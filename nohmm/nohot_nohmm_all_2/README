-- Retroactively added, from memory, chat logs, and e-mails sent --

Thurs Jun 06 2013 Chris Brumbaugh

Working to modify nohot_all_good1 Makefile from using SAM HMM models to using blasr and resulting .sam multiple alignment
for iteration and model surgery since HMM models take too long to generate for what is needed here. Will be comparing
results to original Makefile results to see (relatively) how well this new method works.

Fri Jun 07 2013 Chris Brumbaugh

Replaced HMM_ITER with blasr to generate multiple alignment.

Used previous traning data as reads to blasr and iter0 as the reference to output the multiple sequence alignment to
a .sam file.

Next, used samtools mpileup and generate a variant calls format based on the .sam alignment that blasr created.

Finally used GATK from the Broad Institute to apply the .vcf to the previous iteration as model surgery to apply
changes.

There is an issue generating the filtered .vcf from the raw .vcf derived from the samtools mpileup. Maybe a problem
with the samtools Perl script used to filter the .vcf file?

Sat Jun 08 2013 Chris Brumbaugh

Held Skype call with Kevin. Kevin suggested two papers to read related to the run length encoding that has been considered
as the implementation to try instead of the temporary use of blasr.

Told Kevin about the issue with samtools mpileup not working properly, maybe due to the samtools Perl script. Suggested to
try to do my own filtering if I cannot get the Perl script filtering working.

Mon Jun 10 2013 Chris Brumbaugh

Figured out why the pileup step was failing. bcftools requires a '-' as the input file to actually read from STDIN.

GATK does not like fasta files with spaces in them (not recognized IUPAC character for fasta file format). Used sed
to edit reference file in place to be removed of spaces. (Why are there spaces in the fasta files in the first
place...?)

Downloaded and installed/placed Picard in /projects/compbio as the dictionary generation is required for GATK to
work. Added dictionary generation to Makefile.

Should run to completion now (hopefully).

Tues Jun 11 2013 Chris Brumbaugh

Looks like the Makefile failed to progress past the first iteration. Looking into why it failed (presumably issue
with GATK?).

It appears that GATK in the alternate fasta file it produces with the .vcf file has the header replaced, so the
selection for training that occurs on subsequent iteration has no information in the header line to know what to
select for. Carried over the header from the previous iteration, changing iter0 (previous) to iter1 (current) with
sed, escaping all the regex characters present in the header line (forward slash) for the sed statement to replace
the GATK header line (">1") with the approrpriate header information.

Metric in STAGE 4 fails as it is expecting information from the HMM models from the previous step. Will need to
replace with something more appropriate, as HMM use has been replaced.

Wed Jun 12 2013 Chris Brumbaugh

Read how SAM's HMM NNL score is used as a metric to compare and see if the sequence was generated from that given
model (negative natural log of the probability that a given sequence was generated from the HMM model).

In the Makefile, it appears that the 10th ranked NNL score was (arbirarily?) used to determine which consensus
sequence generated from the seeds will be used to represent a given merged region.

After examining the variant calls format more closely, it looks like there are some form of quality scoring
associated with the modifications represented in the file. Could be useful?

Maybe should look into how many reads map vs unmapped (all reads? training reads?) to the consensus sequences.

Considering using something combining both of the ideas mentioned above? Sent Kevin an email about it.

Will try using the combined reads pool from the training sequences to select the best consensus sequence to move
forward with.

Thurs Jun 13 2013 Chris Brumbaugh

Starting to rewrite STAGE 4 to something more appropriate.

Looks like concatenating gzip files together produces a valid gzip file. Going to merge all PacBio reads from the
last iteration and use as the combined reads pool.

Installed fastx toolkit in order to use fastx_collapser in order to remove any duplicate reads present in the
combined reads pool.

fastx_collapser doesn't like the input from the merged set of reads; it recommends using fasta_formatter to format
the data being input to fastx_collapser. After changing this, fastx_collapser now works. It looks like only the
header information is lost with the fasta_formatter, but that information isn't necessary as this combined pool of
reads will only be used here.

After reading and poking around, it looks like CollectAlignmentSummaryMetric.jar from Picard should have what I would
like to use to select the best consensus sequence (PF_ERROR_RATE).

Used blasr to use the combined pool of reads against each consensus sequence to generate a .sam alignment, then used
samtools to generate a sorted .bam file as input for Picard. The output of CollectAlignmentSummaryMetric.jar seems
like a simple enough text file to parse and get the error rate to sort on.

Able to extract and sort on the error rate, so hopefully this modification should work.

All of the error rates produced are 0 - not sure if this is an issue with something I did earlier, or a
misunderstanding with what the PF_ERROR_RATE produced is.

Fri Jun 14 2013 Chris Brumbaugh

Decided to abandon the use of Picard to generate the summary metric, will generate a metric on my own with a Python
script.

Not decided if whether using some sort of crude score (e.g. total aligned - (substitutions + insertions + deletions +
etc)) or some other form of weighted metric (e.g. percentage, or something else).

Wrote up script with some sort of percentage, using the substitutions, insertions, deletions, soft clip, and hard
clip from the .sam CIGAR string divided by the total bases aligned as the metric to use. Still playing with script in
~cbrumbau/tmp to make sure it works first.

Looks like I tried to build and install pysam, but it didn't work. The pysam install with the setup script doesn't
work because it doesn't view the directory I am trying to install it to (/projects/combio/lib/python2.7) as a valid
library location used by Python 2.7. Will resolve this issue later.

Old code using Picard was removed/modified in the Makefile, so here it is for reference:

<CODE>

# Align the merged pool for each consensus sequence using blasr
# Then get .sam alignment summary statistics with Picard
# blasr only likes .fasta file extension, do something about that
%-iter${ITER}.stats: ${REGION_AB}-iter${ITER}_merged_pool.fasta %-iter${ITER}.fna
	mv $*-iter${ITER}.fna $*-iter${ITER}.fasta
	blasr $< $*-iter${ITER}.fasta \
	    -nproc 4 \
	    -sam \
	    -out $*-iter${ITER}_select_seed.sam
	mv $*-iter${ITER}.fasta $*-iter${ITER}.fna
	samtools view -b $*-iter${ITER}_select_seed.sam \
	| samtools sort $*-iter${ITER}_select_seed.sorted
	java -jar ${PICARD}/AlignmentSummaryMetrics.jar \
	    ADAPTER_SEQUENCE= null \
	    INPUT= $*-iter${ITER}_select_seed.sorted.bam \

# Select the top scoring consensus sequence, looking for lowest alignment error rate with Picard
${REGION_AB}-iter${ITER}.selected-fna: ${SELECT_STATS}
	cat $^ \
	| grep -E '(OUTPUT|^\w)' \
	| awk '{if ($$1 == "#") {printf $$5"\t"} else if ($$1 != "CATEGORY") {print $$14}}' \
	| sort -n -k 2 \
	| head -1 \
	| awk '{print $1}' \
	| sed -e 's/OUTPUT=//' \
	| sed -e 's/.stats/.fna/'
	> $@

</CODE>

Mon Jun 17 2013 Chris Brumbaugh

Was able to install pysam using the bash shell (over using default cshell) to set the environmental variable required
to identify additional Python library paths.

pysam still can't be used in Python. Looks like there were issues with the directory structure regarding how the
library was installed. Instead of the structure seen in Cython/Pyrex (module.egg-info with subdirectory e.g. cython/
or pyrex/) pysam installed with a different structure (module.egg/ directory with EGG-INFO/ and pysam/ subdir).
Adjusting pysam to have the same structure as the other modules (moving module.egg/EGG-INFO/PKG-INFO to
module.egg-info on the top directory and moving pysam up a directory) allowed it to be properly seen my Python 2.7.
Perhaps a newer version of the setup script for 3.x was used? That wouldn't entirely make sense as pysam hasn't been
rewritten for 3.x yet, though.

Decided to go with a crude weighted score to determine which consensus sequence to choose. The score is calculated as
follows:

weighted score = bases aligned - (substitution weight * substitutions + insertion weight * insertions + deletion
weight * deletions + soft clip + hard clip + unmapped bases)

Sent Kevin an email with news that STAGE 4 has been modified and completed. Looks like the Makefile is running
smoothly in region2_3 now.

Tues Jun 18 2013 Chris Brumbaugh

Problems with STAGE 5 have appeared. After looking into the issue with the Python debugger (python2.7 -m pdb), it
seems that the merged regions (region1_2 and region2_3) can't be matched up to each other with blat. The call in
merge-sequences-with-blat using region2 as a guide region to where_longest_common seems to be returning None because
there was no max_dict present. It seems that blat is running correctly, but there are no results to report when
looking for an overlapping subsequence. Sent Kevin an email regarding this issue.

Re-reading the papers Kevin sent me on my tablet while waiting for a response. Not sure if this issue in STAGE 5
means there is an error in my use of GATK to do the model surgery in the iteration stage, or an issue with the merged
pool I am using to select the best consensus sequence to use to represent the merged region in STAGE 4.

-- Retroactive addition end ---

Wed Jun 19 13:26:40 PDT 2013 Chris Brumbaugh

Finished writing up retroactive addtion to this README file. Sending Kevin an email about this.

Plan to try merge-sequences-with-blat without a guide to see the results. Leaning towards thinking the issue lies
with using GATK to perform the model surgery; however, with the samtools pileup format deprecated and samtools
mpileup only generating .vcf/.bcf output, I am trying to consider a simple method to use in order to do the model
surgery.

Thu Jun 20 12:55:39 PDT 2013 Chris Brumbaugh

Held Skype meeting with Kevin. Discussed that the reason STAGE 5 was failing to find a subsequence between the merged
regions is due to the fact that when iterating model surgery on the consensus sequence it is slowly losing length.
Suggested looking at the consensus-from-a2m script with the "--insert be" option and modifying it to work with .sam
alignments in order to resolve the issue. Might need to rework the SAM_ITER stage to no longer need samtools
mpileup and Picard/GATK if going to generate the consensus sequence from the .sam file.

Also discussed some possible ideas regarding the implementation of using this protocol (when complete) on the 10 strains
data set. The problem will be changing from selecting one consensus sequence representative of a merged region to
multiple consensus sequences, so I'll need a way to strip out poorly supported consensus sequences (likely due to
erroneous reads, or an actual strain that's just underrepresented in the data) and some way to rank and select the
consensus sequences to proceed with. Mmerging similar consensus sequences could also be a possible option. Also, it
would be worth noting that looking into whether penalizing for soft clip and hard clip are necessary for the
weighted score and not to count unmapped reads against the consensus sequences when working with the multiple strain
data.

Moved personal builds of libraries and utilities (e.g. blat, fastx_toolkit, pysam, samtools) to
/projects/compbio/programs and ran fixmode on them. Added README.ucsc to specify problems encountered when installing
them and how to work around it.

Fri Jun 21 14:01:45 PDT 2013

Looked at consensus-from-a2m script to implement a version that will work with .sam alignment format.

Rough idea is as follows:

<PSEUDOCODE>

read in alignments one at a time:
	parse CIGAR list for matches
	generate dict of positions with dict of matches and counts, use position offset
	parse CIGAR list for inserts
	keep track of longest insert before given position
		generate dict of positions with longest insert seen in that position?
		how to do this given alignments in .sam format? need to consider offset?
		note: .a2m format uses - as padding to align the alignment to the reference
		note: .a2m uses lowercase for insert, uppercase for match

apply the 'b' and 'e' settings as done in consensus-from-a2m

don't implement random begin end for now

create consensus sequence, going through reference sequence positions and comparing to match dict

</PSEUDOCODE>

Working on implementing this in a new Python script.

Still fixing up the portion that parses information out of the .sam alignment using pysam.

Mon Jun 24 13:08:28 PDT 2013 Chris Brumbaugh

Apparently my account has been moved over to an alumni account, and I have lost access to the Python script I was working on.

Submitted a ticket to IT, sent email to Kevin, should be resolved eventually (hopefully sooner rather than later).

Resolved with help of Eric Shell (eshell@soe.ucsc.edu). Resumed work on consensus-from-sam.

Tue Jun 25 10:03:36 PDT 2013 Chris Brumbaugh

Python 2.7 cannot be found any longer. When attempting to access it:

> /usr/bin/env python2.7
/usr/bin/env: python2.7: No such file or directory

Not sure yet if this has to do with the move to the alumni group or some other recent change with the system.

Looks like on some of the SOE compute servers the file handle for /projects/linuxSW was screwed up due to the file
system upgrade this morning; an IT ticket was submitted regarding the issue. A couple of the servers have the file
mount working properly, so I have moved over to one of those to continue my work.

The .sam file parsing and extraction for alignment matches and insertions seems to be working fine now, moving on to
preparing the inserts that need to be added and condensing the matches into the new consensus sequence.

Added logic to build longest_insert_before dict from the longest_insert dict.

Wed Jun 26 18:07:35 PDT 2013 Chris Brumbaugh

Held meeting with Skype today over Kevin. Discussed implementation of consensus-from-a2m and what features I
should reimplement in consensus-from-sam. The random additions are not necessary and were left in from a
first attemp that did not work as anticipated. Resturctured some of the code in consensus-from-sam to be
more similar to consensus-from-a2m in order to apply some code reuse.

Continued to work on consensus-from-sam and it is about ready to be integrated into the nohmm Makefile.

Thu Jun 27 14:01:31 PDT 2013 Chris Brumbaugh

Completed consensus-to-sam and tested it; it hopefully shouldn't have any major bugs at this point.

Revising Makefile to use consensus-from-sam, old code is here for reference:

<CODE>

HEADER := $(shell grep ">" ${SEED_NAME}-iter${PREV_ITER}.fna | sed -e 's/iter${PREV_ITER}/iter${ITER}/' | sed -e 's/\//\\\//g')

sam_iter:	Pac_reads_in_${SEED_NAME}-iter${PREV_ITER}.blastn.gz \
		${SEED_NAME}-selection-for-train-iter${ITER} \
		${SEED_NAME}-selection-for-train-iter${ITER}-begin \
		${SEED_NAME}-selection-for-train-iter${ITER}-end \
		${SEED_NAME}-oriented_train-iter${ITER}.fasta.gz \
		${SEED_NAME}-iter${ITER}.sam \
		${SEED_NAME}-iter${ITER}.bam \
		${SEED_NAME}-iter${ITER}.sorted.bam \
		${SEED_NAME}-iter${ITER}.flt.vcf \
		${SEED_NAME}-iter${ITER}.fna \
		${SEED_NAME}-sam_iter.DONE

# ...

# Sort BAM file
%-iter${ITER}.sorted.bam: %-iter${ITER}.bam
	samtools sort \
	    $< \
	    $*-iter${ITER}.sorted

# Generate pileup using previous iter for reference to SAM alignment
%-iter${ITER}.flt.vcf: %-iter${ITER}.sorted.bam %-iter${PREV_ITER}.fna
	samtools faidx $*-iter${PREV_ITER}.fna
	samtools mpileup -uf $*-iter${PREV_ITER}.fna $< \
	| bcftools view -cg - \
	| /projects/compbio/bin/scripts/vcfutils.pl varFilter -D100 \
	> $@

# Apply filtered vcf to previous iteration to generate current consensus sequence
# Use GATK from the Broad, insert path to jar
# GATK only likes .fasta or .fa file extension, do something about that
# GATK doesn't like spaces in fasta files
# Generate necessary dict file for reference with Picard
# Fix improper header output from GATK
%-iter${ITER}.fna: %-iter${ITER}.flt.vcf %-iter${PREV_ITER}.fna
	sed -e '/>/n;s/ //g' $*-iter${PREV_ITER}.fna > $*-iter${PREV_ITER}.fasta
	samtools faidx $*-iter${PREV_ITER}.fasta
	java -jar ${PICARD}/CreateSequenceDictionary.jar \
	    R= $*-iter${PREV_ITER}.fasta \
	    O= $*-iter${PREV_ITER}.dict
	java -Xmx2g -jar ${GATK}/GenomeAnalysisTK.jar \
	    -R $*-iter${PREV_ITER}.fasta \
	    -T FastaAlternateReferenceMaker \
	    -o $@ \
	    --variant $<
	sed -i -e 's/>1/${HEADER}/' $@

.PRECIOUS:	\
		%-selection-for-train-iter${ITER} \
		%-selection-for-train-iter${ITER}-begin \
		%-selection-for-train-iter${ITER}-end \
		%-oriented_train-iter${ITER}.fasta.gz \
		%-iter${ITER}.sam \
		%-iter${ITER}.sorted.bam \
		%-iter${ITER}.flt.vcf \

</CODE>

Apparently when using a .bam with pysam, it wants the .bam file to be sorted and indexed. Added steps to do this to
the Makefile.

Fri Jun 28 12:40:05 PDT 2013 Chris Brumbaugh

There was an error in consensus-from-sam, where separate indices for the reference and for the alignment were not
properly kept. A reference index was added, and now the Makefile runs smoothly.

Looking into either replacing the initial blastall to filter out good reads for training with blasr or increasing the
number of processor cores blastall uses to near max number for the system it is running on. That should net some sort
of a speed increase, but it appears that most of the time is the actual subsequent filtering and selecting of
training reads before a multiple alignment is produced using blasr.

Mon Jul  1 12:08:06 PDT 2013 Chris Brumbaugh

Added multiple core support to the Makefile, now blastall and blasr will use multiple cores depending on the machine
it is running on.

Finished adjusting the insertion steps to try and counteract the final consensus sequence being too long compared to
the real sequence. The resulting consensus sequence was shorted, but still was too long for the alignment when
comparing to the real sequence.

Tue Jul  2 12:45:13 PDT 2013 Chris Brumbaugh

Adjusted the blasr options to be more sensitive when aligning the training reads and now using a percentage idenity
threshold on the reads when aligned to the reference. Trying this change in ../nohot_nohmm_all_3.

Changed the reads used in the iteration stage into the alignment reads (a lot more reads than the training set).
Trying this change in ../nohot_nohmm_all_4.

Will be removing all these directories and condensing to one directory after the results from all of the runs are
compared to each other.

Fri Jul  5 14:06:34 PDT 2013 Chris Brumbaugh

Forgot to add enties as I was too busy working on getting training-from-sam and the Makefile working.

Talked with Kevin on Wednesday about what to do for planning and what to expect when modifying the new protocol to
work on data containing 10 strains as opposed to a single strain. Will plan on adding choosing which reads to apply
to a consensus sequence before attempting an alignment, looking into mixture models and how to train mixtures, and
considering what to do to replace stage 5, as region 2 is not a variable region and should be common to all strains,
so it can't be used as a guide to merge region1_2 and region2_3.

Additionally, direction-from-blast isn't required as blasr (and the future replacement I will write to deal with
homopolymers) do not need all the reads in the same direction as working with HMMs in SAM did.

training-from-sam is complete, and the new training appears to proceed well, but currently there is an issue
proceeding from the trained consensus sequence to the alignment to generate the final consensus sequence for that
pass. It appears that this issue is occuring in the direction-from-blast in the orientation target for the alignment
reads, so it may be caused at this part or possibly with the dependencies required for this target. As far as I can
tell, the input appears to be fine, so it may be an issue with direction-from-blast. Replacing and/or modifying this
script might have to be done sooner than expected.

Wed Jul 10 17:56:41 PDT 2013 Chris Brumbaugh

Run with corrected protocol finished on Monday, taking a bit over 24 hours. Discussed results with Kevin on Tuesday.

Although the results are mostly correct now with the added training iteration step, there are issues with the bases at
the ends of the consensus sequence. This could potentially due to the alignment algorithm used in blasr, or an issue
with too few end training reads (as using too many would have made running SAM to generate HMMs too slow).

In order to try and speed up the protocol (where most of the time is spent using blastn and orienting the reads), 
replacement of these portions with a single blasr against all reads and then the training and generation of the 
consensus sequence would be handled by a single script taking the reads that aligned to the inital blasr.

Regarding the issue with the ends, first it should be determined if it is a read issue or an alignment issue with blasr. 
This can be determined by using blasr to align all the reads against the "real" sequence and looking at the resulting 
multiple alignment. If it is indeed an issue with blasr using local alignments, then the ends can be "enhanced" by
looking for reads that align at the beginning and ends, and using the optional user fields added to the final 
alignment by blasr to see if there is more of the read before (at the beginning) or after (at the end) and incorporate 
this into the consensus sequence for the extensions.

Fri Jul 12 16:42:31 PDT 2013 Chris Brumbaugh

The replacement of blastn and the orientation script have been sucessfully replaced with one call to blasr in the
Makefile and the subsequent selection of reads, training, and alignment handled by training-and-consensus-from-sam.

The selection for the training and aligning reads is done by splitting the reference up into bins based on the number
of reads required. If the number of reads is larger than the number of bases in the reference, bins are set to the
number of bases in the reference and multiple passes over the bins are used until the number of reads requested is
gathered. If the number of reads is larger than the number of reads available from the initial alignment, the maximum
number of reads (all the reads in the alignment) is provided. If neither of these cases are true, for the first half
of each bin alignments are provided that start in those positions and for the last half of each bin alignments are
provided that end in those positions in order to try to provide an even coverage of reads for training and alignment.

Previously, the method tried to provide training and alignment reads by picking a random alignment and then chosing
subsequent alignments from each side of the alignment until the entire reference had been traversed, repeating this
the number of requested times. The issue with this method was that the total number of reads from the selected
alignments varied wildy and as such wasn't a reliable method to use to select reads.

Currently the option used to extend the ends appears to be implemented and working; however, the amount of bases it
adds onto the ends from the reads seems to be excessively long. It finds the longest read extending out from the
reference from all the alignments within the specified end length option. Perhaps there should be either some way
to select quality extensions or of a reasonable size, or maybe an entirely different method should be used.

Tue Jul 16 23:50:24 PDT 2013 Chris Brumbaugh

The results from the intitial run as well as a subsequent run where the number of reads used for training
and aligning were increased by five times were faster, but not nearly as accurate as the previous Makefile.
Most of the errors were indels that were homopolymer errors. This could be attributed to an alignment
issue, read selection issue, or model surgery issue.

Assuming that it could be attributed to a read selection issue, the criteria for read selection was altered
and instead of randomly selecting an alignment and associated read from each bin, only the best alignments
are now selected from each bin.

Thu Jul 18 13:22:24 PDT 2013 Chris Brumbaugh

Started implementing RLE addition to model surgery as suggested by Kevin during yesterday's Skype meeting.

This addition should help address the homopolymer errors in the final consensus sequence as it appears to
be caused by the fact that since the reads are no longer being oriented in the same direction, the indels
are being split across the homopolymer regions from reads on the forward and reverse strands.

Aside from old and new functionality being added as options to training-and-consensus-from-sam, the old
(inconsistent) method used to select the reads for training and alignment is removed and placed here
for reference:

<CODE>

# Old code selecting a spread over the reference
# The number of reads it produced per iteration varied too much
# May want to go back and inspect/fix at later date
reads_for_train = "selection_for_train_" +  str(os.path.splitext(args.reference)[0]) + ".fasta"
reads_for_align = "selection_for_align_" +  str(os.path.splitext(args.reference)[0]) + ".fasta"
for read_select in ["train", "align"]:
	if read_select == "train":
		logging.info("Selecting reads for training...")
		coverage = args.coverage_train_reads
	if read_select == "align":
		logging.info("Selecting reads for aligning...")
		coverage = args.coverage_align_reads
	positions_select_start = positions_start
	positions_select_end = positions_end
	select_reads = list()
	random.seed()
	# Get evenly distributed reads for coverage
	for iter in range(coverage):
		# Select random start alignment
		length = 0
		start = 0
		while length == 0:
			start = random.randint(0, len(reference)-1)
			length = len(positions_select_start[start])
		# Set start and end values
		index_start = start-1
		index_end = positions_select_start[start][0][1]
		# Remove start from list
		select_reads.append(positions_select_start[start].pop(0))
		# Extend to start
		while index_start > 0:
			if len(positions_select_end[index_start]) == 0:
				index_start -= 1
			else:
				start = positions_select_end[index_start][0][1]
				align = positions_select_end[index_start].pop(0)
				select_reads.append(align)
		# Extend to end
		while index_end < len(reference):
			if len(positions_select_start[index_end]) == 0:
				index_end += 1
			else:
				end = positions_select_start[index_end][0][1]
				align = positions_select_start[index_end].pop(0)
				select_reads.append(align)
	select_reads = set(select_reads)
	if read_select == "train":
		logging.info("Total reads for training: " + str(len(select_reads)))
		file_name = reads_for_train
	if read_select == "align":
		logging.info("Total reads for aligning: " + str(len(select_reads)))
		file_name = reads_for_align
	# Write reads to file
	record_dict = SeqIO.index(args.reads, "fasta")
	SeqIO.write([record_dict[qname[0]] for qname in select_reads], file_name, "fasta")

</CODE>

Mon Jul 22 17:11:11 PDT 2013 Chris Brumbaugh

It appears that using the RLE in the model surgery doesn't help the results when using both
the 1x reads parameters as well as the 5x read parameters. Kevin has suggested using both
counts before and after collapsing the homopolymers for the RLE, I will have to think of
the best way to store this (probably store full information in RLE index?) and what to do
with the full information (group mirror bases for forward/reverse? wouldn't storing and
using the full information not be RLE? what valid operations are there for a homopolymer:
keep length, extend length, shorten length?).

I have started modifying training-and-consensus-from-sam to be able to use an alternate
read selection scheme closer to the original protocol to try and see if the issue with
the final consensus sequence might have to do with the current read selection scheme.

Fri Jul 26 13:18:37 PDT 2013 Chris Brumbaugh

So far the best results have come from using 1x read parameters with RLE v1 with match
and indel prob of 0.6.

After implementing orienting the training and alignment reads, it appears that these
settings are not ideal as the Makefile fails on merging in stage 5. Either I have not
implemented the orientation correctly, or end extensions are now necessary, or that the
parameters are no longer optimal for the new script.

Currently I am working on integrating the changes Kevin suggested (apply matches to RLE
using expected number of bases, consider inserts between homopolymers as well as inserts
within homopolymers) by instead carrying the full information to the subroutines that
apply the matches, deletions, and insertions, and reduce the information to the RLE
coordinates so I still have full access to the unencoded information as well. Once fully
implemented, I will refer to these changes as RLE v2.

Mon Jul 29 15:43:55 PDT 2013 Chris Brumbaugh

Finished applying the changes for RLE v2. Currently testing the new changes with the
read orientation, which should hopefully improve the final consensus sequence. Depending
on the results, fixing and reenabling the extend option may be required.

Tue Jul 30 11:56:55 PDT 2013 Chris Brumbaugh

Working on reimplementing the extend option to work similarly to how it worked in the
original protocol.

Removing old initial consensus code and old training code from the
training-and-consensus-from-sam script.

<CODE>

# Generate initial consensus sequence from alignment
# Takes a long time, redo (e.g. with pileup) or skip
# Disable for now
"""
logging.info("Performing initial consensus training with alignment...")
initial_consensus = train_consensus(args.limit, reference, None, args.min_coverage, args.min_prob, args.min_coverage_ins, args.min_prob_ins, args.min_coverage_del, args.min_prob_del, align_file=str(args.alignment), rle=args.rle)
"""

# Extend intial consensus sequence if requested
# Extensions added are far too long to be useful
# Disable for now
"""
if args.extend:
	# Find extension from start
	logging.info("Searching for extension to apply to beginning...")
	extend_start_list = list()
	for start_index in range(args.end_length):
		length = len(positions_start[start_index])
		if length > 0:
			# Get length of read that extends beyond reference
			for align_index in range(length):
				(read_name, ref_end, XS, XQ) = positions_start[start_index][align_index]
				align_pos = start_index + 1
				size = XS - align_pos
				read_pos = XS - align_pos
				extend_start_list.append((size, read_name, read_pos))
	# Get longest extension from available alignments
	logging.info("Selecting longest extension for beginning...")
	(max_size, max_read_name, max_read_pos) = sorted(extend_start_list, key=lambda tup: tup[0], reverse=True)[0]
	if max_size > 0:
		start_read = max_read_name
		start_read_pos = max_read_pos
	else:
		start_read = None
		start_read_pos = None
	# Find extension from end
	logging.info("Searching for extension to apply to end...")
	extend_end_list = list()
	for end_index in range(-1, -(args.end_length+1), -1):
		length = len(positions_end[end_index])
		if length > 0:
			# Get length of read that extends beyone reference
			for align_index in range(length):
				(read_name, align_pos, XE, XQ) = positions_end[end_index][align_index]
				ref_end = end_index
				size = XQ - XE + ref_end
				read_pos = XE + abs(ref_end)
				extend_end_list.append((size, read_name, read_pos))
	# Get longest extension from available alignments
	logging.info("Selecting longest extension for end...")
	(max_size, max_read_name, max_read_pos) = sorted(extend_end_list, key=lambda tup: tup[0], reverse=True)[0]
	if max_size > 0:
		end_read = max_read_name
		end_read_pos = max_read_pos
	else:
		end_read = None
		end_read_pos = None
	# Get actual extension bases from reads
	logging.info("Extracting extensions from reads...")
	record_dict = SeqIO.index(args.reads, "fasta")
	if start_read:
		start_extend = str(record_dict[start_read].seq)[0:start_read_pos]
	if end_read:
		end_extend = str(record_dict[end_read].seq)[0:end_read_pos]
	# Add to consensus sequence (reference)
	logging.info("Extending start with: " + start_extend)
	logging.info("Extending end with: " + end_extend)
	reference = start_extend + reference + end_extend
"""

</CODE>

Tue Aug  6 11:56:50 PDT 2013 Chris Brumbaugh

RLE v2 has been improved where there are fewer bases lost during the training iterations
due to implementing rounding before converting the expected lengths to ints and modifying
the match/extend function to not apply a deletion if the expected length of the homopolymer
is less than 1; however, a couple hundred bases in length are still lost each training.

This suggests that the expected length is still being undercounted and/or there is an issue
with the alignment algorithm used itself (blasr).

Fri Aug 16 14:12:28 PDT 2013 Chris Brumbaugh

Swapping out blasr for aligning and replacing with blastn is taking longer than expected,
with the need to write up a script to convert the blastn default output to sam format.

Currently I am looking into the blasr scoring to make sure the issue does not lie there by
following Kevin's suggestion and coming up with my own crude scoring scheme for the CIGAR
alignments and using that to select the reads instead of using blasr scores.

